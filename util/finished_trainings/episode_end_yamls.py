"""This script offers utilities to load & process statistical training outputs, i.e. episode-end YAML files."""
import re
from pathlib import Path
from typing import Any

import pandas as pd
import yaml

from util.nested_dicts import flatten_nested_dict


_EPISODE_END_YAML_FILE_PATTERN = re.compile(r".+\.yaml\.(\d+)$", flags=re.IGNORECASE)
_LOGS_ENV_INDEX_FOLDER_PATTERN = re.compile(r"^logs_env-index-(\d+)$", flags=re.IGNORECASE)


def _load_env_index_yaml_files(yaml_folder_path: Path) -> pd.DataFrame:
    """Searches a directory for episode-end YAML files and loads them into a DataFrame."""
    assert yaml_folder_path.is_dir(), f"'{yaml_folder_path}' does not exist or is no directory!"
    episodes__yaml_file_paths: dict[int, Path] = {}
    for _f in yaml_folder_path.glob("*"):
        _match = _EPISODE_END_YAML_FILE_PATTERN.match(_f.name)
        if _match is None:
            continue
        _episode_number = int(_match.group(1))
        assert _episode_number not in episodes__yaml_file_paths.keys()
        episodes__yaml_file_paths[_episode_number] = _f
    assert len(episodes__yaml_file_paths) > 0, f"The given path '{yaml_folder_path}' doesn't contain any suitable YAMLs"
    # Check if any episode YAML is missing
    assert min(episodes__yaml_file_paths.keys()) == 1
    assert max(episodes__yaml_file_paths.keys()) == len(episodes__yaml_file_paths), \
        f"It seems that there is at least one episode-YAML missing in folder '{yaml_folder_path}'!"
    # Let's load our metrics
    metrics = []
    for _episode, _file_path in episodes__yaml_file_paths.items():
        with open(_file_path, mode="r", encoding="utf-8") as f:
            _raw_yaml_metrics: dict[str, Any] = yaml.safe_load(f)
        if _raw_yaml_metrics is None:
            print(f"{yaml_folder_path.name}: Skipping episode {_episode}, because YAML is empty!")
            continue
        if "done" in _raw_yaml_metrics and _raw_yaml_metrics["done"] is False:
            print(f"{yaml_folder_path.name}: Skipping episode {_episode}, because 'done == False'")
            continue
        # --> Flatten nested dicts, if necessary (e.g. emissions-subdict)
        _flattened_episode_end_metrics = flatten_nested_dict(_raw_yaml_metrics)
        _flattened_episode_end_metrics["episode"] = _episode
        metrics += [_flattened_episode_end_metrics]
    df_episode_based_metrics = pd.DataFrame(metrics).set_index("episode").sort_index()
    return df_episode_based_metrics


def load_single_trial_results(single_trial_path: Path, moving_average_window_size: int = None) -> pd.DataFrame:
    """
    Loads episode-end YAML files from all 'logs_env-index-*' folders within a single trial. The index of the returned
    DataFrame corresponds to the timestamp the regarding episode was finished.

    @param single_trial_path: Path of the single training trial, whose episode-end YAMLs we wish to load.
    @param moving_average_window_size: If not None, a moving average window is applied to each of the loaded data
                                       columns. The resulting "mean", "median", "std" are inserted as new columns.
    @return: Dataframe with the loaded episode-end metrics that was written during training. Index is the timestamp
             that was present during training. Each row stands for one finished episode throughout the training process.
    """
    # Localize all "logs"-subfolders. Each of them was generated by one environment
    assert single_trial_path.is_dir(), f"Trial path '{single_trial_path}' does not exist or is no folder!"

    _cache_file_path = single_trial_path / "merged-episode-end-metrics.cache.tsv"
    if _cache_file_path.is_file():
        _df = pd.read_csv(_cache_file_path, sep="\t", encoding="utf-8", index_col="timestamp").sort_index()
    else:
        env_indices__paths: dict[int, Path] = {}
        for _f in single_trial_path.glob("**"):
            _match = _LOGS_ENV_INDEX_FOLDER_PATTERN.match(_f.name)
            if _match is None:
                continue
            _env_index = int(_match.group(1))
            assert _env_index not in env_indices__paths.keys()
            env_indices__paths[_env_index] = _f
        assert len(env_indices__paths) > 0, f"Given trial folder '{single_trial_path}' does not hold any logs folders!"

        # Load YAML data from each of the env-index subfolders
        _env_indices__data_frames = \
            {idx: _load_env_index_yaml_files(yaml_folder_path=path) for idx, path in env_indices__paths.items()}
        for _env_index, _df in _env_indices__data_frames.items():
            assert "timestamp" in _df.columns, f"Trial path '{single_trial_path}': " \
                                               f"Env index '{_env_index}' does not provide timestamp values!"

        # Merge all dataframes into one & sort by timestamp
        _df = pd.concat(_env_indices__data_frames.values())
        _df = _df.set_index("timestamp").sort_index()

        # Write to cache, if possible
        try:
            _df.to_csv(_cache_file_path, sep="\t", encoding="utf-8", header=True, index=True)
        except OSError:
            pass  # Could not write cache. That's okay --> let's ignore it!

    # If desired, determine moving averages/medians/stds over the acquired data
    if moving_average_window_size is not None:
        _column_names = [c for c in _df.columns]
        for _metric in _column_names:
            _window = _df[_metric].rolling(window=moving_average_window_size)
            _df[_metric + "_mean"] = _window.mean()
            _df[_metric + "_median"] = _window.median()
            _df[_metric + "_min"] = _window.min()
            _df[_metric + "_std"] = _window.std()
    return _df
